\chapter{June 18, 2024}

\section{Cross Products}

Aside from the dot product, there is one more product to consider between vectors. This, of course, is the cross product, an operation that results in a vector output and does not follow the typical rules of multiplication that the dot product follows.

\begin{definition}[Vector Cross Product]
    In $\RR_{3}$ space, the cross product of $\vec{u} = \left<a_{1}, a_{2}, a_{3}\right>$ and $\vec{v} = \left<b_{1}, b_{2}, b_{3}\right>$ is
    \[\vec{u} \times \vec{v} = \det \begin{bmatrix} \hat{\imath} & \hat{\jmath} & \hat{k} \\ a_{1} & a_{2} & a_{3} \\ b_{1} & b_{2} & b_{3} \end{bmatrix}.\]
\end{definition}

\begin{remark}
    Take note that the vector cross product is only generally defined in three dimensions (and seven, but that's beyond the scope of this course).
\end{remark}

\begin{example}
    Find the cross product of $\vec{u} = \left<4, 3, 1\right>$ and $\vec{v} = \left<5, -7, 2\right>$.

    \begin{soln}
        Using our above definition, we see that
        \begin{align*}
            \vec{u} \times \vec{v} &= \det \begin{bmatrix}
                \hat{\imath} & \hat{\jmath} & \hat{k} \\
                4 & 3 & 1 \\
                5 & -7 & 2
            \end{bmatrix} \\
            &= \left<3 \cdot 2 - (-7) \cdot 1, -4 \cdot 2 + 5 \cdot 1, 4 \cdot (-7) - 5 \cdot 3\right> \\
            &= \left<13, -3, -43\right>.
        \end{align*}
    \end{soln}
\end{example}

\begin{remark}
    Another important note is that the cross product always returns a vector that is perpendicular to both input vectors (unless they are parallel, in which the zero-vector is returned).
\end{remark}

\begin{example}
    Prove that in general, the cross product returns an orthogonal vector to both input vectors.

    \begin{proof}
        Let vector $\vec{u} = \left<a_{1}, a_{2}, a_{3}\right>$ and $\vec{v} = \left<b_{1}, b_{2}, b_{3}\right>$. From our original definition of the cross product we derive that
        \[\vec{u} \times \vec{v} = \left<a_{2}b_{3} - a_{3}b_{2}, a_{3}b_{1} - a_{1}b_{3}, a_{1}b_{2} - a_{2}b_{1}\right>.\]
        Now consider $\vec{u} \cdot (\vec{u} \times \vec{v})$, which we wish to show is zero. It follows that
        \[\vec{u} \cdot (\vec{u} \times \vec{v}) = a_{1}a_{2}b_{3} - a_{1}a_{3}b_{2} + a_{2}a_{3}b_{1} - a_{1}a_{2}b_{3} + a_{1}a_{3}b_{2} - a_{2}a_{3}b_{1}.\]
        Miraculously, all the terms cancel out. Without loss of generality, we apply the same logic to $\vec{v}$ and we are done.
    \end{proof}
\end{example}

Going back to the above definition of a cross product, we can arrive at a trigonometric association for the cross product that is similar to the one established for the dot product.

\begin{proposition}
    Two vectors $\vec{u} = \left<a_{1}, a_{2}, a_{3}\right>$ and $\vec{v} = \left<b_{1}, b_{2}, b_{3}\right>$ satisfy
    \[\magnitude{\vec{u} \times \vec{v}} = \magnitude{\vec{u}} \magnitude{\vec{v}} \sin\theta\]
    where $\theta$ is the angle between $\vec{u}$ and $\vec{v}$.
\end{proposition}

In the beginning, I note that the cross product has slightly different multiplicative properties than the dot product. Because of this, I list the specific properties below.

\begin{itemize}
    \item \textbf{Anticommutivity:} $\vec{a} \times \vec{b} = -(\vec{a} \times \vec{b})$.
    \item \textbf{Scalar Multiplication:} $(k\vec{a}) \times \vec{b} = \vec{a} \times (k\vec{b}) = k(\vec{a} \times \vec{b})$.
    \item \textbf{Distributivity:} $\vec{a} \times (\vec{b} + \vec{c}) = \vec{a} \times \vec{b} + \vec{a} \times \vec{c}$.
    \item \textbf{Non-Associativity:} In general, $(\vec{a} \times \vec{b}) \times \vec{c} \neq \vec{a} \times (\vec{b} \times \vec{c})$.
\end{itemize}

Now, we explore an example that utilizes the trigonometric representation above.

\begin{example}[Volume of a Parallelepiped]
    Prove that the volume of a parallelepiped is
    \[V = \vec{u} \cdot (\vec{v} \times \vec{w})\]
    where the vectors represent the three sets of parallel sides.

    \begin{proof}
        Rearranging the above we have
        \begin{align*}
            V &= \magnitude{\vec{u}} \magnitude{\vec{v} \times \vec{w}} \cos\theta \\
            &= \magnitude{\vec{u}} \magnitude{\vec{v}} \magnitude{\vec{w}} \cos\theta \sin\alpha
        \end{align*}
        where $\theta$ is the angle between $\vec{u}$ and the altitude $h$ between the bases determined by sides in the directions of $\vec{v}$ and $\vec{w}$ and $\alpha$ is the angle between $\vec{v}$ and $\vec{w}$. It is easy to see that $h = \magnitude{\vec{u}}\cos\theta$ and the area of the base $b = \magnitude{\vec{v}} \magnitude{\vec{w}} \sin\alpha$. So we attain $V = bh$, the standard volume formula.
    \end{proof}
\end{example}

\section{Lines}

Lines in 3-dimensional space are not written in the same way that they are in two-dimensional space. In fact, they must be paramaterized or written using symmetric equation (which we get to later).

\begin{definition}
    A line in $\RR_{3}$ space is written in the form $\vec{p} + t\vec{u}$ where $\vec{p}$ represents a position vector, $\vec{u}$ represents a direction vector, and $t$ is a varied parameter.
\end{definition}

\begin{remark}
    Often, $\vec{u}$ in the above is a unit vector, but this is not necessary.
\end{remark}

\begin{theorem}[Distance between a Point and a Line in 3D]
    Given an arbitrary point $P$ in $\RR_{3}$ space not on line $\ell$ and a point $Q$ on $\ell$, let the vector from $P$ to $Q$ be $\vec{u}$ and $\vec{v}$ be a vector parallel to $\ell$. The shortest distance from $P$ to $\ell$ is
    \[d = \frac{\magnitude{\vec{u} \times \vec{v}}}{\magnitude{\vec{v}}}.\]
\end{theorem}

\begin{example}[Distance between a Point and a Plane in 3D]
    Find a general formula for the distance between a point and a plane not containing that point in $\RR_{3}$ space.

    \begin{soln}
        Let our point be $P$ and let $Q$ be an arbitrary point on plane $\mathcal{P}$. Let $\vec{u}$ be the vector from $P$ to $Q$ and let $\vec{v}$ be a vector parallel to the normal vector of $\mathcal{P}$. It follows that the distance is the magnitude of the projection of $\vec{u}$ onto $\vec{v}$, which is defined by
        \[d = \abs{\vectorproj{\vec{v}}{\vec{u}}} = \frac{\abs{\vec{u} \cdot \vec{v}}}{\magnitude{\vec{v}}}.\]
    \end{soln}
\end{example}

\section{The Jacobian}

Now that's the end of all the main material that was covered in class, but the professor also brought up the topic of Jacobians, so I include them here. In essence, the Jacobian is a matrix that approximates how a vector-valued function changes at a point. A rigorous definition is below.

\begin{definition}[The Jacobian Matrix]
    A function $f : \RR_{n} \to \RR_{m}$ has Jacobian matrix
    \[J = \begin{bmatrix}
        \frac{\partial f_{1}}{\partial x_{1}} & \cdots & \frac{\partial f_{1}}{\partial x_{n}} \\
        \vdots & & \vdots \\
        \frac{\partial f_{m}}{\partial x_{1}} & \cdots & \frac{\partial f_{m}}{\partial x_{n}}
    \end{bmatrix}.\]
\end{definition}

While rigorous, the above definition is likely a bit overkill for what we may cover in the class. So, just in case, I include a simpler example.

\begin{example}[A Simple Jacobian]
    Given a function $f(x, y, z) = xyz + x^{2}y^{2}z^{2}$, find its Jacobian matrix.

    \begin{soln}
        We know that the Jacobian matrix is
        \[J = \begin{bmatrix}
            \frac{\partial f}{\partial x} & \frac{\partial f}{\partial y} & \frac{\partial f}{\partial z}
        \end{bmatrix}\]
        from our definition above. What's left is to calculate the partial derivatives, yielding an answer of
        \[J = \begin{bmatrix}
            yz + 2xy^{2}z^{2} & xz + 2x^{2}yz^{2} & xy + 2x^{2}y^{2}z
        \end{bmatrix}.\]
    \end{soln}
\end{example}

\begin{remark}
    Now, we have not yet gone over partial derivatives. However, to evaluate them you only must consider change in one variable. The derivative rules from previous calculus classes still apply, but all other variables are treated as constants.
\end{remark}